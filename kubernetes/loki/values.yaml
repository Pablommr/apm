loki:
  server:
    log_level: warn
    log_format: json

  ui:
    enabled: true

  schemaConfig:
    configs:
    - from: "2024-04-01"
      store: tsdb
      object_store: s3
      schema: v13
      index:
        prefix: loki_index_
        period: 24h

  storage:
    type: s3
    bucketNames:
      chunks: ""
      ruler: ""
      admin: ""
    s3:
      region: us-east-1
      s3forcepathstyle: false

  ingester:
    chunk_encoding: snappy
    max_chunk_age: 24h
    lifecycler:
      ring:
        # 'memberlist' é usado para que os ingesters se descubram
        # e formem o anel de consistência.
        replication_factor: 2 # Mudar conforme quantidade de pods no Statefulset
        kvstore:
          store: memberlist
    wal:
      enabled: true
      checkpoint_duration: 5m0s
      dir: /var/loki/wal
      flush_on_shutdown: true
      replay_memory_ceiling: 3GB
  
  pattern_ingester:
    enabled: true
  
  limits_config:
    ingestion_rate_mb: 100
    ingestion_burst_size_mb: 200
    max_line_size: 1048576
    allow_structured_metadata: true
    volume_enabled: true
    reject_old_samples: true
    reject_old_samples_max_age: 168h # 7 dias
    retention_period: 0h

  compactor:
    retention_enabled: true
    delete_request_store: s3

  ruler:
    enable_api: true

  querier:
    max_concurrent: 4

  queryFrontend:
    max_query_parallelism: 64

serviceAccount:
  create: true
  annotations:
    "eks.amazonaws.com/role-arn": "arn:aws:iam:::role/LokiServiceAccountRole"

deploymentMode: Distributed

ingester:
  kind: StatefulSet
  # IMPORTANTE: O número de réplicas agora deve ser um múltiplo 
  # do número de AZs que você usa. Ex: 2 réplicas por zona em 3 zonas = 6 réplicas.
  # O Helm chart vai dividir isso entre as zonas.
  replicas: 6
  zoneAwareReplication:
    enabled: true
  podAnnotations:
    karpenter.sh/do-not-disrupt: "true"
  resources:
    requests:
      cpu: "1"
      memory: "15Gi"
    limits:
      cpu: "2"
      memory: "16Gi"

compactor:
  replicas: 3
  nodeSelector:
    karpenter.sh/nodepool: general-ondemand #On-demand em prod
  podAnnotations:
    karpenter.sh/do-not-disrupt: "true"

distributor:
  replicas: 3
  maxUnavailable: 2
  nodeSelector:
    karpenter.sh/nodepool: general
  podAnnotations:
    karpenter.sh/do-not-disrupt: "true"
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: loki
              app.kubernetes.io/component: distributor
          topologyKey: "kubernetes.io/hostname"

querier:
  replicas: 4
  maxUnavailable: 2
  nodeSelector:
    karpenter.sh/nodepool: general
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: loki
              app.kubernetes.io/component: querier
          topologyKey: "kubernetes.io/hostname"
  resources:
    requests:
      cpu: "3"
      memory: "10Gi"
    limits:
      cpu: "4"
      memory: "12Gi"
  autoscaling:
    enabled: true
    minReplicas: 4
    maxReplicas: 8
    targetCPUUtilizationPercentage: 70
    targetMemoryUtilizationPercentage: 50

queryFrontend:
  replicas: 6
  maxUnavailable: 2
  nodeSelector:
    karpenter.sh/nodepool: general

queryScheduler:
  replicas: 2
  nodeSelector:
    karpenter.sh/nodepool: general

indexGateway:
  replicas: 2
  maxUnavailable: 1
  nodeSelector:
    karpenter.sh/nodepool: general

ruler:
  replicas: 1
  maxUnavailable: 1
  nodeSelector:
    karpenter.sh/nodepool: general

# Desabilitando componentes não utilizados no modo distribuído
minio:
  enabled: false
backend:
  replicas: 0
read:
  replicas: 0
write:
  replicas: 0
singleBinary:
  replicas: 0